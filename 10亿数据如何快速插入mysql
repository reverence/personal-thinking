#############10亿条数据快速插入mysql#######################
1,每条数据转为数据库数据大约1k。

2,数据都存在txt文档中，需要进行解析后才能存到数据库中。

3,所有数据对应的文档都存在Hdfs或S3分布式文件存储里。

4,数据被分到100个文件中，通过文件的后缀确定这些数据的批次。

5,要求保证有序导入，数据尽可能不重复。

6,存储的数据库是MySQL。
==========================个人思考====================================
1,首先想到的第一点，数据表先不要建索引，等插入完成后再建索引！
2,批量插入insert into values(),一次发送条数根据max_allowed_packet来定，比如4M的话，可以一次3000条插入
3,10亿条数据如果都放入一张表，不太现实，写入都是有瓶颈的，需要分库分表
4,分多少库？多少表？首先考虑下单表存储多少条数据比较合适。单表建议的上限一般上限在2kw左右,怎么来的呢？
MySQL底层索引结构采用的是B+树，插入的数据都是存储在聚簇索引的叶子节点下，
本次插入的数据差不多1k左右的大小，而MySQL数据的页大小为16k，
这也就意味者一个叶子节点最多可以存储16条数据。
而非叶子节点存储的数据主键和指向叶子的指针，假设我们主键类型为bigint，即8字节，
而指针在innoDB中是为6字节，那么一个非叶子节点可以记录的数据计算公式为:
页大小*1024/(主键大小+指针大小)=16*1024/(8+6)≈1170
就是说一个非叶子节点大约可以表示1170条数据，而MySQL一张数据表存储数据建议的层级为3层左右，
所以一张表最大存储的数据量为2000w
1170 * 1170 * 16 =21902400≈2000w
又因为数据分配在100个文件，且需要保证有序导入，所以可以设计100张表来存，每张条1kw数据
可以使用10库10表，每张表对应一个文件，单表顺序插入，保证顺序
5，如何保证可靠性？10亿数据的插入，万一中间有点小问题或者抖动导致异常中断，该怎么办？
建立一个任务进度表，记录每个文件的写入进度，每次提交后，往任务表写一条记录，任务表可以这样设计
dbId--数据库序号，tbId--数据表序号，fileName--文件名，offset-读取到多少条了，
每次批量写完后更新对应的数据读取记录，同时成功才算写入成功。失败后，下次再根据offset读取数据插入
6，如何快速读取数据？每个文件大约10G，不可能一次性将所有文件都读入内存。可以考虑java nio filechannel读取，
但这种缓冲区读的方式不太好统计读取到多少条。可以考虑bufferedReader读取，一行一行处理
7,最终可以概括为：
a,分10库10表
b，建立100个任务（线程），每个任务对应一个文件读取，写入对应的表
c，每个任务从对应的文件中读取数据，3000条为一批insert，同时update任务进度表
d，check下任务表，如果有中断的任务，从中断的offset开始，重新读取数据insert

